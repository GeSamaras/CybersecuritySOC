Logs serve as invaluable records of past events, providing essential insights to address these questions. By preserving an archive of historical activities, we can bolster our security posture and protect our digital assets more effectively.

|   |   |
|---|---|
|![Various types of digital logs](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/220bb7889266ea7e29322a0e4a09c19c.svg)|A comprehensive understanding of logs is crucial for identifying patterns and mitigating potential threats.<br><br>As manually examining the vast amount of log data generated by numerous systems and applications can be challenging, it is vital to grasp the intricacies of log analysis and become acquainted with the available tools and techniques.<br><br>Log analysis tools and methods empower individuals to interpret historical events and establish a reliable source of historical evidence, streamlining the processing and scrutiny of log data. This efficiency facilitates prompt detection and response to potential incidents or significant events.|

By analysing logs as records of historical activities, individuals and organisations can gain essential knowledge, enhancing their overall awareness and preparedness across a wide range of situations.

Learning Objectives

This room covers how logs can be used to record an adversary's actions, the tools and techniques needed to perform log analysis, and the significance of effectively collecting and analysing logs.

- Understand the importance of logs as a historical activity record for identifying and mitigating potential threats
- Explore various types of logs, logging mechanisms and collection methods across multiple platforms
- Gain hands-on experience detecting and defeating adversaries through log analysis

Recommended Reading

This room will primarily focus on logs and log files using a Linux-based VM, for those interested in Windows-specific event logs, completing the [Windows Event Logs](https://tryhackme.com/room/windowseventlogs) room is recommended.


In the Heart of Data: Logs

Just as a physical tree's rings reveal its life story – indicating good years with thick curls and challenging ones with thin – a digital log provides a historical record of system activity.

Both embody a fundamental principle of growth over time and serve as living records in their respective domains – physical and digital.

In the digital world, every interaction with a computer system – from authentication attempts, granting authorisation, accessing a file, and connecting to a network to encountering a system error – will always leave a digital footprint in the form of logs.

|   |   |
|---|---|
|![Components of a digital log](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/624b4e33606f8b0e1dc2ff887eb70c3f.svg)|Logs are a record of events within a system. These records provide a detailed account of what a system has been doing, capturing a wide range of events such as user logins, file accesses, system errors, network connections, and changes to data or system configurations.<br><br>While the specific details may differ based on the type of log, a log entry usually includes the following information:<br><br>- A timestamp of when an event was logged<br>- The name of the system or application that generated the log entry<br>- The type of event that occurred<br>- Additional details about the event, such as the user who initiated the event or the device's IP address that generated the event<br><br>This information is typically stored in a **log file**, which contains aggregated entries of what occurred at any given time on a system.|

However, since digital interactions are continuous and fast-paced, the log file's size may exponentially grow depending on the activities logged on a system.

The True Power of Logs: Contextual Correlation

A single log entry may seem insignificant on its own. But when log data is aggregated, analysed, and cross-referenced with other sources of information, it becomes a potent investigation tool. Logs can answer critical questions about an event, such as:

- **What** happened?
- **When** did it happen?
- **Where** did it happen?
- **Who** is responsible?
- **Were** their actions **successful**?
- **What** was the result of their action?

The following hypothetical scenario can illustrate this aspect. Suppose a student allegedly accessed inappropriate content on a University network. By reviewing the logs, a systems administrator could then answer the following:

|Question|Answer|
|---|---|
|What happened?|An adversary was confirmed to have accessed SwiftSpend Financial's GitLab instance.|
|When did it happen?|Access started at 22:10 on Wednesday, September 8th, 2023.|
|Where did it happen?|The event originated from a device with an IP address of 10.10.133.168 within the VPN Users' segment (10.10.133.0/24).|
|Who is responsible?|Upon examining the network logs, it was observed that the device, identified by the User-Agent "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/115.0", was allocated the IP address 10.10.133.168.|
|Were they successful?|Yes, since an API Key was found to be publicly exposed on the GitLab instance. Moreover, the web proxy logs confirm that the adversary device reached _gitlab.swiftspend.finance_ and maintained access through their uploaded web shell.|
|What is the result of their action?|The adversary achieved remote code execution on _gitlab.swiftspend.finance_ and performed post-exploitation activities.|

The example above emphasises how logs are instrumental in piecing together a complete picture of an event, thereby enhancing our understanding and ability to respond effectively.


Logs are more than mere records of historical events; they can serve as a guiding compass. They are invaluable resources that, when skillfully leveraged, can enhance system diagnostics, cyber security, and regulatory compliance efforts. Their role in keeping a record of historical activity for a system or application is crucial.

Log Analysis Process

Log analysis involves Parsing, Normalisation, Sorting, Classification, Enrichment, Correlation, Visualisation, and Reporting. It can be done through various tools and techniques, ranging from complex systems like Splunk and ELK to ad-hoc methods ranging from default command-line tools to open-source tools.

|   |   |
|---|---|
|![Image Representation of a Data Source](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/bb3dac37904b48660e1ad524832a6383.svg)|Data Sources<br><br>Data Sources are the systems or applications configured to log system events or user activities. These are the origin of logs.|
|![Image Representation of Log Parsing](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/b92ca5757d85ab49ee32cb0217b84b3a.svg)|Parsing<br><br>Parsing is breaking down the log data into more manageable and understandable components. Since logs come in various formats depending on the source, it's essential to parse these logs to extract valuable information.|
|![Image Representation of Log Normalisation](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/6d50d78e6c09b878bc4bf6aebf73b7a9.svg)|Normalisation<br><br>Normalisation is standardising parsed data. It involves bringing the various log data into a standard format, making comparing and analysing data from different sources easier. It is imperative in environments with multiple systems and applications, where each might generate logs in another format.|
|![Image Representation of Log Sorting](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/26eb3e15a848f9249e2951ef44bbcedb.svg)|Sorting<br><br>Sorting is a vital aspect of log analysis, as it allows for efficient data retrieval and identification of patterns. Logs can be sorted by time, source, event type, severity, and any other parameter present in the data. Proper sorting is critical in identifying trends and anomalies that signal operational issues or security incidents.|
|![Image Representation of Log Classification](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/907a1ff6f5a4c8fd0c0250282a23568f.svg)|Classification<br><br>Classification involves assigning categories to the logs based on their characteristics. By classifying log files, you can quickly filter and focus on those logs that matter most to your analysis. For instance, classification can be based on the severity level, event type, or source. Automated classification using machine learning can significantly enhance this process, helping to identify potential issues or threats that could be overlooked.|
|![Image Representation of Log Enrichment](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/2f5c66bfdad44c7f2fd5dd211a7541fc.svg)|Enrichment<br><br>Log enrichment adds context to logs to make them more meaningful and easier to analyse. It could involve adding information like geographical data, user details, threat intelligence, or even data from other sources that can provide a complete picture of the event.<br><br>Enrichment makes logs more valuable, enabling analysts to make better decisions and more accurately respond to incidents. Like classification, log enrichment can be automated using machine learning, reducing the time and effort required for log analysis.|
|![Image Representation of Log Correlation](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/0a909d3f679c458f65ef08d5770389fc.svg)|Correlation<br><br>Correlation involves linking related records and identifying connections between log entries. This process helps detect patterns and trends, making understanding complex relationships between various log events easier. Correlation is critical in determining security threats or system performance issues that might remain unnoticed.|
|![Image Representation of Log Visualisation](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/f19a2dcb7e6af13d489590b4b13c6c3f.svg)|Visualisation<br><br>Visualisation represents log data in graphical formats like charts, graphs, or heat maps. Visually presenting data makes recognising patterns, trends, and anomalies easier. Visualisation tools provide an intuitive way to interpret large volumes of log data, making complex information more accessible and understandable.|
|![Image Representation of Log Reporting](https://tryhackme-images.s3.amazonaws.com/user-uploads/63da722f2d207d0049da10b1/room-content/315f0456ea5f0130ac6a05a7d17153f3.svg)|Reporting<br><br>Reporting summarises log data into structured formats to provide insights, support decision-making, or meet compliance requirements. Effective reporting includes creating clear and concise log data summaries catering to stakeholders' needs, such as management, security teams, or auditors. Regular reports can be vital in monitoring system health, security posture, and operational efficiency.|

Log Analysis Tools

Security Information and Event Management (SIEM) tools such as Splunk or Elastic Search can be used for complex log analysis tasks.

However, in scenarios where immediate data analysis is needed, such as during incident response, Linux-based systems can employ default tools like `cat`, `grep`, `sed`, `sort`, `uniq`, and `awk`, along with `sha256sum` for hashing log files. Windows-based systems can utilise [EZ-Tools](https://ericzimmerman.github.io/#!index.md) and the default cmdlet `Get-FileHash` for similar purposes. These tools enable rapid parsing and analysis, which suits these situations.

Additionally, proper acquisition should be observed by taking the log file's **hash during collection** to ensure its admissibility in a court of law.

Therefore, it is imperative not only to log events but also to ensure their integrity, that they are analysed, and any lessons obtained from the logs be learned, as the safety and efficiency of an organisation can depend on them.

Log Analysis Techniques

Log analysis techniques are methods or practices used to interpret and derive insights from log data. These techniques can range from simple to complex and are vital for identifying patterns, anomalies, and critical insights. Here are some common techniques:

**Pattern Recognition:** This involves identifying recurring sequences or trends in log data. It can detect regular system behaviour or identify unusual activities that may indicate a security threat.

**Anomaly Detection:** Anomaly detection focuses on identifying data points that deviate from the expected pattern. It is crucial to spot potential issues or malicious activities early on.

**Correlation Analysis:** Correlating different log entries helps understand the relationship between various events. It can reveal causation and dependencies between system components and is vital in root cause analysis.

**Timeline Analysis:** Analysing logs over time helps understand trends, seasonalities, and periodic behaviours. It can be essential for performance monitoring and forecasting system loads.

**Machine Learning and AI:** Leveraging machine learning models can automate and enhance various log analysis techniques, such as classification and enrichment. AI can provide predictive insights and help in automating responses to specific events.

**Visualisation:** Representing log data through graphs and charts allows for intuitive understanding and quick insights. Visualisation can make complex data more accessible and assist in identifying key patterns and relationships.

**Statistical Analysis:** Using statistical methods to analyse log data can provide quantitative insights and help make data-driven decisions. Regression analysis and hypothesis testing can infer relationships and validate assumptions.

These techniques can be applied individually or in combination, depending on the specific requirements and complexity of the log analysis task. Understanding and using these techniques can significantly enhance the effectiveness of log analysis, leading to more informed decisions and robust security measures.

Working with Logs: Practical Application

Working with logs is a complex task requiring both comprehension and manipulation of data. This tutorial covers two scenarios. The first is handling unparsed raw log files accessed directly via an open-source [Log Viewer](https://github.com/sevdokimov/log-viewer) tool. This method allows immediate analysis without preprocessing, which is ideal for quick inspections or preserving the original format.

The second scenario focuses on creating a parsed and consolidated log file using Unix tools like `cat`, `grep`, `sed`, `sort`, `uniq`, and `awk`. It involves merging, filtering, and formatting logs to create a standardised file. Accessible through the [Log Viewer](https://github.com/sevdokimov/log-viewer) tool, this consolidated file offers a clear and efficient view of the data, aiding in identifying patterns and issues.

These approaches highlight the flexibility and significance of log analysis in system diagnostics and cyber security. Whether using raw or parsed logs, the ability to compile, view, and analyse data is vital for an organisation's safety and efficiency.p